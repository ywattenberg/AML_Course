# Group 'Deepposterior'
In order to perform feature selection, we mainly focused on the correlation between the features. We made use of the DropCorrelatedFeatures function from feature_engine.selection, the SmartCorrelatedSelection function from feature_engine.selection and the corr function from pandas. Moreover, we used the DropConstantFeatures to throw constant or quasi-constant features away. In addition, we also looked at the correlation between features and labels. We drop those features that have not a high correlation with the labels. Now the next task was to fill the NaN values according to some strategy. But before doing so, we first scaled our data using the StandardScaler from sklearn. Next, we decided to fill the NaNs in the data using the median-strategy. Next, we used the sklearn SelectKBest function to select features according to the k highest scores. We used K=150. To deal with the outliers we used the Empirical-Cumulative-distribution-based Outlier Detection (ECOD). Now our data was ready to be fed to a regression model. We built an ensemble of multiple models (SVR, LGBMRegressor, EMVR, ExtraTreesRegressor, CatBoostRegressor). We determined the parameters for each of the 5 models by using a GridSearchCV from sklearn. We combined the models in a StackingRegressor to get the final model to train the data on. After training, we used our trained model to make the final prediction on the test data and exported the results in a csv file.

# Group 'Randomname'
In this project, we are asked to predict the patient's age from MRI features. Our team started out with three steps of data preprocess. First, we conducted feature selection on the given samples: we used the random forest regressor to select the most important 98 features out of the 1212 features given in the original data set. Next, we removed 70 outlier samples using isolation forest (outlier detection), and then filled the missing values (i.e. the NAs in the data set) using the KNN imputer. After that, we tried a bunch of different models on our cleaned data, including the gradient boosting regressor, the random forest regressor,  and the gaussian process regressor with different kernels. We used a cross validation number of 10. We ultimately decided to use the gaussian process regressor with a rotational quadratic kernel, which gives out the best performance on the training data set, with a score of 0.748 and standard deviation of 0.027. The model achieved 0.739 point on the leaderboard.

# Group 'Amlllll'
We use regression to predict the brain age from a given feature set. First, we use SimpleImputer on the raw training and test data to fill in the missing values with the column-wise medians. We standardize the training and the test data using RobustScaler from sklearn, which transforms the data to the interquartile range.
We then use SelectKBest from sklearn to select the best 175 features from the training data using feature_selection.f_regression as the score_func. Using the original unprocessed data we reselect these 175 feature columns from the training and test data. We will now process the data sets using a different approach.
We again perform data scaling using RobustScaler so that our data would be more robust to outliers, and impute the missing nan-values using KNNImputer. This now fills in the missing values using the mean from the 15 nearest neighbors found in our reduced and scaled data. To achieve a better regression fit, we remove the outliers in the data using IsolationForest with the max sampling number 100. We choose the points that are not deemed to be outliers, with anomaly score 1.
We initialize our custom class for regression using GaussianProcessRegressor from sklearn with RationalQuadratic as the kernel. We have the processed, anomaly reduced training data and the processed test data for the class. We fit the training data and the labels using our GP regressor and then predict the labels for the test features and read them into a csv file.
