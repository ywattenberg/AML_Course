We first start by imputing the data with the sklearn Simple Imputer using the median strategy. We then start by normalizing our data with the QuantileTransformer. After normalizing we filter outliers with a Bayesian Gaussian Mixture model by using a threshold and discarding all rows which are below the given threshold. For feature selection, we are using k-best selection with f-regression as the scoring function. Our regression model is a Gradient Boosting Regressor with squared loss. Each of these steps were cross-validated. For GBR we looked at ranges from 100 to 1000, for k-best we looked at ranges from 150 to 250 and for the components of the BGM we looked at 10 to 500 and thresholds between 1% and 20%. Our best results were achieved with 1000 GBR regression steps, 200 BGM components with a threshold of 1% and 200 k-best features. 

Other methods that we tried were using PCA combined with BGM model, but we did not achieve higher validation scores with that technique. We also tried using a Random Forest Regressor, but we did not achieve higher validation scores with that technique either. Further, we also tried using a neural network, but could not achieve a better score.